# -*- coding: utf-8 -*-
"""euro_stoxx_train.ipynb의 사본

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Mik6-HE6ip__kwcR36JAQLlLoOTnFp42
"""

from stage1.data import add_feature

__all__ = ['euro_lstm']

def euro_lstm(cfg):
    from os.path import join as opj
    import pandas as pd
    import numpy as np
    import yfinance as yf
    from pandas_datareader import data as pdr
    import matplotlib.pyplot as plt
    import ta

    train = pd.read_csv(opj(cfg.base.data_dir, "euro_train.csv"))
    val = pd.read_csv(opj(cfg.base.data_dir, "euro_valid.csv"))
    test = pd.read_csv(opj(cfg.base.data_dir, "euro_test.csv"))

    # sma = pd.read_csv('/content/drive/MyDrive/ITStudy/파이널프젝/euro_stoxx_sma.csv')

    # exchange = pd.read_csv('/content/drive/MyDrive/ITStudy/파이널프젝/exchange_usd_euro_2017_2021.csv')
    # vstoxx = pd.read_csv('/content/drive/MyDrive/ITStudy/파이널프젝/vstoxx_2017_2021.csv')
    # quality= pd.read_csv('/content/drive/MyDrive/ITStudy/파이널프젝/quality_2017_2021.csv')
    # dollar = pd.read_csv('/content/drive/MyDrive/ITStudy/파이널프젝/dollar_2017_2021.csv')


    raw_train = pd.concat([train,val,test])


    #date열 str에서 datetime형으로 변환
    raw_train['date'] = pd.to_datetime(raw_train['date'])

    # target_year=2021
    # validation = raw_train[raw_train['date'].dt.year==target_year]




    train = add_feature(train)
    val = add_feature(val)
    test = add_feature(test)


    """# LSTM"""



    # save original 'returns' prices for later
    # original_returns = raw_train['target'].values

    # separate dates for future plotting
    dates = pd.to_datetime(raw_train['date'])

    raw_train.set_index("date",inplace=True)


    # variables for training
    cols = list(raw_train)[0:14]


    # new dataframe with only training data
    stock_data = raw_train[cols].astype(float)

    # normalize the dataset
    from sklearn.preprocessing import StandardScaler, MinMaxScaler
    scaler = StandardScaler()
    scaler = scaler.fit(stock_data)
    stock_data_scaled = scaler.transform(stock_data)

    # from sklearn.model_selection import train_test_split
    # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=246, random_state=123)
    # X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=248, random_state=123)

    raw_train.reset_index(inplace=True)

    target_year=2021
    train = raw_train[raw_train['date'].dt.year<target_year]
    len(train)

    target_year=2021
    validation = raw_train[raw_train['date'].dt.year==target_year]
    len(validation)

    target_year=2022
    test = raw_train[raw_train['date'].dt.year==target_year]
    len(test)

    # split to train data and test data
    n_train = 1025
    train_data_scaled = stock_data_scaled[0: n_train]
    train_dates = dates[0: n_train]

    n_validation = n_train + 258
    val_data_scaled = stock_data_scaled[n_train: n_validation]
    val_dates = dates[n_train: n_validation]

    n_test = n_validation

    test_data_scaled = stock_data_scaled[n_test:]
    test_dates = dates[n_test:]

    len(val_data_scaled)

    len(test_data_scaled)

    import numpy as np
    # data reformatting for LSTM
    pred_days = 30  # prediction period - 3months
    seq_len = 50   # sequence length = past days for future prediction.
    input_dim = train_data_scaled.shape[1]  # input_dimension = ['close', 'open', 'high', 'low', 'rsi', 'MACD_12_26', 'MACD_sign_12_26', 'hband', 'mavg', 'lband', 'CSI', 'target']

    trainX = []
    trainY = []
    valX = []
    valY = []
    testX = []
    testY = []

    # 추론 날짜 (base_date 출력을 위한)
    val_dates_for_infer = []
    test_dates_for_infer = []
    # val_dates, test_dates는 pd.Series로 되어 있고 numpy datetime으로 되어 있다.
    val_dates = val_dates.astype('string')
    test_dates = test_dates.astype('string')
    for i in range(seq_len, n_train - pred_days + 1):
        trainX.append(train_data_scaled[i - seq_len:i, 0:train_data_scaled.shape[1]])
        trainY.append(train_data_scaled[i + pred_days - 1:i + pred_days, 0])

    for i in range(seq_len, len(val_data_scaled) - pred_days + 1):
        valX.append(val_data_scaled[i - seq_len:i, 0:val_data_scaled.shape[1]])
        valY.append(val_data_scaled[i + pred_days - 1:i + pred_days, 0])
        # base_dates추가
        val_dates_for_infer.append(val_dates[i + pred_days - 1:i + pred_days].values[0])

    for i in range(seq_len, len(test_data_scaled) - pred_days + 1):
        # print("testX :", testX)
        # print("testX.shape :", len(testX))
        testX.append(test_data_scaled[i - seq_len:i, 0:test_data_scaled.shape[1]])
        testY.append(test_data_scaled[i + pred_days - 1:i + pred_days, 0])
        # base_dates추가
        test_dates_for_infer.append(test_dates[i + pred_days - 1:i + pred_days].values[0])

    trainX, trainY = np.array(trainX), np.array(trainY)
    valX, valY = np.array(valX), np.array(valY)
    testX, testY = np.array(testX), np.array(testY)

    print(trainX.shape, trainY.shape)
    print(testX.shape, testY.shape)
    print(valX.shape, valY.shape)


    # LSTM model
    import numpy as np
    import pandas as pd
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import LSTM, Dense
    from tensorflow.keras.optimizers import Adam
    from sklearn.preprocessing import StandardScaler
    import matplotlib.pyplot as plt

    
    model = Sequential()
    model.add(LSTM(64, input_shape=(trainX.shape[1], trainX.shape[2]),
                return_sequences=True))
    model.add(LSTM(32, return_sequences=False))
    model.add(Dense(trainY.shape[1]))

    # specify your learning rate
    learning_rate = 0.001
    # create an Adam optimizer with the specified learning rate
    optimizer = Adam(learning_rate=learning_rate)
    # compile your model using the custom optimizer
    model.compile(optimizer=optimizer, loss='mse')

    validation_data = (valX,valY)

    if cfg.base.mode=="train":
        # import matplotlib.pyplot as plt
        # try:
        #     model.load_weights('./lstm_weights_2.h5')
        #     print("Loaded model weights from disk")
        # except:
            # Fit the model
        history = model.fit(trainX, trainY, epochs=30, batch_size=4, validation_data=validation_data,
                        verbose=1) #validation - 과적합 방지 & loss 가 작을수록 좋은 모델이니 그것을 선택함
        # Save model weights after training
        model.save_weights(opj(cfg.base.output_dir, 'lstm_weights_6.h5'))


    else:

        model.load_weights(opj(cfg.base.output_dir, 'lstm_weights_6.h5'))
        val_pred = model.predict(valX)
        test_pred = model.predict(testX)

        # 결과 저장
        pd.DataFrame(data={"date":val_dates_for_infer, cfg.base.task_name:val_pred.reshape(-1,)}).to_csv(opj(cfg.base.output_dir, f"{cfg.base.task_name}_prediction_21.csv"), index=False)
        pd.DataFrame(data={"date":test_dates_for_infer, cfg.base.task_name:test_pred.reshape(-1,)}).to_csv(opj(cfg.base.output_dir, f"{cfg.base.task_name}_prediction_22.csv"), index=False)
       
